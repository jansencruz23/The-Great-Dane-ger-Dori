================================================================================
                    DORI - PRESENTATION SCRIPT
                    5-Minute Demo & Q&A Prep
                    Google Developer Groups (GDG)
================================================================================

TOTAL TIME: 5 MINUTES
- Introduction: 45 seconds
- Problem Statement: 30 seconds  
- Solution Overview: 45 seconds
- Live Demo: 2 minutes
- Technical Highlights: 45 seconds
- Closing: 15 seconds

================================================================================
                         PRESENTATION SCRIPT
================================================================================

[SLIDE 1 - TITLE SLIDE]
------------------------------------------------------------------------------
TIME: 0:00 - 0:45 (45 seconds)
------------------------------------------------------------------------------

"Good [morning/afternoon] everyone! I'm [YOUR NAME] from The Great Dane-ger 
team, and today I'm excited to present DORI - an AI-powered augmented reality 
assistant designed to help people with dementia recognize faces and remember 
their loved ones.

Yes, the name is inspired by the lovable forgetful fish from Finding Nemo - 
because just like Dory, our app helps people who struggle with memory to 
navigate their world with a little help from technology."


[SLIDE 2 - THE PROBLEM]
------------------------------------------------------------------------------
TIME: 0:45 - 1:15 (30 seconds)
------------------------------------------------------------------------------

"Imagine not recognizing your own family members. For over 55 million people 
worldwide living with dementia, this is their daily reality. They experience:

‚Ä¢ Anxiety when approached by 'strangers' who are actually loved ones
‚Ä¢ Social isolation because interactions become overwhelming
‚Ä¢ Frustration from repeatedly asking 'Who are you?'
‚Ä¢ Caregiver burnout from constant reintroductions

Current solutions like photo albums are passive - they require the patient to 
remember to use them. We needed something PROACTIVE."


[SLIDE 3 - SOLUTION: DORI]
------------------------------------------------------------------------------
TIME: 1:15 - 2:00 (45 seconds)
------------------------------------------------------------------------------

"DORI is our solution - a Flutter mobile app that uses:

1. REAL-TIME FACE RECOGNITION 
   Using TensorFlow Lite and ML Kit running entirely on-device for privacy

2. AUGMENTED REALITY OVERLAYS
   Floating bubbles appear next to recognized faces showing names, 
   relationships, and recent conversation summaries

3. AI-POWERED MEMORY
   Google Gemini generates warm, natural summaries of past interactions
   like 'This is Maria, your daughter. Yesterday she visited and you 
   talked about your grandchildren's birthday party.'

4. EMPATHETIC DESIGN
   Instead of overwhelming patients, we ask gentle questions like 
   'How do you feel about this person?' with options 'I feel safe' 
   or 'I feel unsure' - respecting their comfort level.

Let me show you how it works!"


[LIVE DEMO]
------------------------------------------------------------------------------
TIME: 2:00 - 4:00 (2 minutes)
------------------------------------------------------------------------------

** DEMO SCRIPT **

[Open the app - Splash Screen appears]
"Here's our beautiful splash screen with glassmorphism effects and the Dori 
logo. The jade green theme was chosen because studies show it has calming 
effects on dementia patients."

[Login as Patient]
"I'll login as a patient user. The app has two modes - one for patients 
and one for caregivers who manage face databases."

[Patient Home Screen]
"This is the patient's home screen. Simple, clean, and not overwhelming. 
The main action is 'Start Remembering' - notice we avoid clinical language."

[Tap 'Start Remembering' - Camera opens]
"Now the magic happens. The camera opens and starts scanning for faces in 
real-time. You can see the time and location displayed - this helps with 
temporal orientation, a common challenge for dementia patients."

[Point camera at a person / use a photo]
"Watch what happens when the camera detects a face..."

[IF RECOGNIZED]
"The app recognized this person! See the AR bubble floating next to their 
face? It shows their name, relationship, and a summary of recent interactions.
The bubble follows the face smoothly as they move."

[IF UNKNOWN FACE - Show the prompt]
"For unknown faces, after a few seconds, a gentle prompt appears asking 
'How do you feel about this person?' with options 'I feel safe' or 
'I feel unsure'. This empathetic approach respects the patient's autonomy.

If they feel safe, we start the enrollment process using voice input.
If they feel unsure, that face is added to a blocklist and won't trigger 
prompts again - preventing anxiety from repeated questions about strangers."

[Show Recording/Transcription if time permits]
"When a known person is detected, the app automatically transcribes the 
conversation. Later, Google Gemini summarizes this into warm, human-readable 
recaps."

[Show Daily Recap screen briefly]
"Patients can also view daily recaps - 'Today you met with Maria for 
15 minutes. You discussed the weather and upcoming family dinner.'"

[Return to home]


[SLIDE 4 - TECHNICAL ARCHITECTURE]
------------------------------------------------------------------------------
TIME: 4:00 - 4:45 (45 seconds)
------------------------------------------------------------------------------

"Let me briefly highlight our technical architecture:

FRONTEND: 
‚Ä¢ Flutter for cross-platform development
‚Ä¢ Provider for state management  
‚Ä¢ Custom AR overlay widgets

AI/ML STACK:
‚Ä¢ TensorFlow Lite with FaceNet-512 for on-device face embeddings
‚Ä¢ Google ML Kit for real-time face detection
‚Ä¢ Google Gemini for conversation summarization

BACKEND:
‚Ä¢ Firebase Authentication with role-based access
‚Ä¢ Cloud Firestore for real-time data sync
‚Ä¢ Firebase Storage for face images

KEY DESIGN DECISIONS:
‚Ä¢ All face recognition runs ON-DEVICE for privacy
‚Ä¢ Cosine similarity with 0.75 threshold for accurate matching
‚Ä¢ Frame-rate optimizations for smooth 10 FPS bubble tracking"


[SLIDE 5 - CLOSING]
------------------------------------------------------------------------------
TIME: 4:45 - 5:00 (15 seconds)
------------------------------------------------------------------------------

"DORI represents our vision of technology that's not just smart, but 
compassionate. We're helping millions of dementia patients stay connected 
with the people who matter most.

Thank you! I'm happy to answer any questions."

[END OF PRESENTATION]


================================================================================
                    TECHNICAL Q&A PREPARATION
================================================================================

Below are anticipated technical questions with detailed answers.

--------------------------------------------------------------------------------
Q1: "How does the face recognition work without internet?"
--------------------------------------------------------------------------------

ANSWER:
"Great question! We use TensorFlow Lite running entirely on-device. Here's 
the process:

1. ML Kit detects face bounding boxes in the camera feed
2. We crop and align the face region from the YUV camera buffer
3. The cropped face is fed into our FaceNet-512 TFLite model
4. The model outputs a 512-dimensional embedding vector
5. We compare this embedding against stored known faces using cosine similarity
6. If similarity exceeds 0.75 threshold, it's a match

No face data ever leaves the device - only metadata like names and 
interaction logs go to Firebase."


--------------------------------------------------------------------------------
Q2: "Why did you choose Flutter over native development?"
--------------------------------------------------------------------------------

ANSWER:
"We chose Flutter for several reasons:

1. CROSS-PLATFORM: Single codebase for iOS, Android, and web
2. PERFORMANCE: Flutter compiles to native ARM code, crucial for real-time 
   face processing
3. HOT RELOAD: Rapid development iteration for UI tweaks
4. RICH ECOSYSTEM: Excellent packages for camera, TFLite, Firebase integration
5. WIDGET SYSTEM: Perfect for building our AR overlay components

The camera package gives us direct access to YUV buffers, enabling optimized 
face cropping without full image conversion."


--------------------------------------------------------------------------------
Q3: "How accurate is the face recognition? What about false positives?"
--------------------------------------------------------------------------------

ANSWER:
"We achieve approximately 95% accuracy in controlled conditions. Here's how 
we prevent false positives:

1. THRESHOLD TUNING: 0.75 cosine similarity threshold - empirically tested
2. MARGIN REQUIREMENT: Best match must be at least 0.025 better than 
   second-best to prevent confusion between similar faces
3. MULTI-ANGLE ENROLLMENT: We capture 5 poses (center, left, right, up, down) 
   during enrollment for robust templates
4. TEMPORAL PERSISTENCE: Recognition must persist for 2+ seconds before 
   triggering actions - prevents flickering

For unknown faces, we require 3 seconds of continuous detection before 
prompting enrollment."


--------------------------------------------------------------------------------
Q4: "How do you handle privacy concerns with storing face data?"
--------------------------------------------------------------------------------

ANSWER:
"Privacy was our top priority:

1. ON-DEVICE PROCESSING: Face detection and embedding extraction happens 
   locally - raw images never leave the device
2. FIREBASE SECURITY: Firestore rules ensure users can only access their 
   own patient data
3. ROLE-BASED ACCESS: Caregivers can only see patients explicitly linked 
   to them via QR code
4. EMBEDDING STORAGE: We store 512-dimension vectors, not reconstructable 
   face images
5. USER CONTROL: Patients can block faces they feel 'unsure' about

We're also exploring encrypted embedding storage for future versions."


--------------------------------------------------------------------------------
Q5: "Why Gemini for summarization instead of a local model?"
--------------------------------------------------------------------------------

ANSWER:
"Gemini provides significant advantages for our use case:

1. CONTEXT UNDERSTANDING: It understands nuance in conversations better 
   than local models
2. WARM TONE: We prompt it to generate 'warm, compassionate' summaries 
   suitable for dementia patients
3. MEMORY EFFICIENCY: On-device LLMs would be too heavy for continuous 
   camera processing
4. CONTINUOUS IMPROVEMENT: Gemini improves over time without app updates

We use the 'gemini-2.5-flash' model for fast, cost-effective responses.
Conversation transcripts are minimal (just text), so API costs are low."


--------------------------------------------------------------------------------
Q6: "How does the AR overlay follow faces so smoothly?"
--------------------------------------------------------------------------------

ANSWER:
"We use a combination of techniques:

1. LERP SMOOTHING: Position updates use linear interpolation with a 0.3 
   factor - formula: newPos = oldPos + 0.3 * (targetPos - oldPos)
2. FRAME SKIPPING: During prompting, we run face detection every 3rd frame 
   to reduce CPU load while maintaining smooth UI at 10 FPS
3. COORDINATE TRANSFORMATION: We convert face bounding box coordinates 
   to screen space considering camera orientation
4. POSITION CLAMPING: Bubbles stay within screen bounds

This gives us smooth 30+ FPS UI while doing heavy ML processing."


--------------------------------------------------------------------------------
Q7: "What happens if the person moves out of frame during enrollment?"
--------------------------------------------------------------------------------

ANSWER:
"Great edge case! We handle this gracefully:

1. We continuously detect faces during enrollment prompting
2. If no face is detected for 3 consecutive frames, enrollment automatically 
   cancels
3. The bubble disappears smoothly
4. No data is saved for incomplete enrollments

This prevents frustrating half-completed enrollments and respects the 
natural flow of real-world interactions."


--------------------------------------------------------------------------------
Q8: "How do you handle multiple faces in the frame?"
--------------------------------------------------------------------------------

ANSWER:
"We support up to 5 simultaneous faces:

1. ML Kit returns bounding boxes for all detected faces
2. Each face gets its own embedding extracted
3. Each embedding is matched against known faces independently
4. AR overlays render for each recognized face with unique keys
5. Tracking IDs from ML Kit help maintain consistent identity across frames

For unknown faces, we only prompt enrollment for the first detected unknown 
to avoid overwhelming the patient."


--------------------------------------------------------------------------------
Q9: "What's your database schema?"
--------------------------------------------------------------------------------

ANSWER:
"We use Firestore with three main collections:

USERS:
- id, email, name, role (patient/caregiver), linkedPatients[], createdAt

KNOWN_FACES:
- id, patientId, name, relationship, imageUrl
- embeddings: { center: [], left: [], right: [], up: [], down: [] }
- lastSeen, createdAt

ACTIVITY_LOGS:
- id, patientId, faceId, startTime, endTime
- transcription, summary, durationSeconds, createdAt

Firestore's real-time sync enables instant updates when caregivers add 
new faces from another device."


--------------------------------------------------------------------------------
Q10: "What are the device requirements?"
--------------------------------------------------------------------------------

ANSWER:
"We've optimized for accessibility:

MINIMUM:
- Android 5.0+ (API 21) or iOS 12+
- 2GB RAM
- Camera with autofocus
- ARM processor (for TFLite)

RECOMMENDED:
- Android 10+ or iOS 14+
- 4GB RAM
- Neural Engine / NPU for faster inference

The app is lightweight - APK is under 50MB despite including the TFLite model.
Frame processing adapts to device capability."


--------------------------------------------------------------------------------
Q11: "How do you handle poor lighting conditions?"
--------------------------------------------------------------------------------

ANSWER:
"Face detection quality degrades in low light, but we've added mitigations:

1. ML Kit's face detector is fairly robust in varied lighting
2. We use 'accurate' performance mode for better detection
3. Face detection confidence threshold is set to 0.2 (permissive)
4. The 3-second debounce for unknown faces prevents false triggers

Future improvements could include prompting users to find better lighting 
or using device flash for enrollment captures."


--------------------------------------------------------------------------------
Q12: "What was the biggest technical challenge?"
--------------------------------------------------------------------------------

ANSWER:
"Honestly, the biggest challenge was COORDINATE TRANSFORMATION between 
camera image space and screen space, especially supporting both portrait 
and landscape orientations.

Camera images come in YUV format with different dimensions than the screen. 
Face bounding boxes are in image coordinates. We need to:
1. Account for camera sensor rotation
2. Scale to screen dimensions  
3. Mirror for front camera
4. Handle orientation changes

We spent significant time debugging AR overlays appearing in wrong 
positions before getting the math right!"


================================================================================
                         END OF SCRIPT
================================================================================

TIPS FOR PRESENTERS:
‚Ä¢ Practice the demo beforehand with a willing volunteer
‚Ä¢ Have backup screenshots/video in case of technical issues
‚Ä¢ Speak slowly during technical sections
‚Ä¢ Make eye contact with the audience, not just the screen
‚Ä¢ Be enthusiastic - this technology genuinely helps people!

Good luck with your presentation! üê†
